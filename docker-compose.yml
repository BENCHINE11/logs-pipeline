services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks: [ bdnet ]

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on: [ zookeeper ]
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    networks: [ bdnet ]

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=bd
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
    networks: [ bdnet ]

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CLUSTER_NAME=bd
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on: [ namenode ]
    ports:
      - "9864:9864"
    volumes:
      - datanode:/hadoop/dfs/data
    networks: [ bdnet ]

  spark:
    image: apache/spark:3.5.1
    container_name: spark
    user: "0"
    depends_on: [ kafka, namenode, datanode ]
    working_dir: /app
    volumes:
      - ./spark-streaming:/app/spark-streaming
      - ./spark-batch:/app/spark-batch
      - ./sql:/app/sql
      - ./.cs-cache:/root/.cache/coursier
      - ./.sbt-cache:/root/.sbt
      - ./.ivy-cache:/root/.ivy2
      - ./.ivy-cache:/home/spark/.ivy2
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=logs_raw
      - HDFS_OUT=hdfs://namenode:8020/datalake/logs/curated
      - CHECKPOINT=hdfs://namenode:8020/datalake/logs/_chk/curated
    command: [ "bash", "-lc", "sleep infinity" ]
    ports:
      - "4040:4040"   # Spark UI (quand un job tourne)
    networks: [ bdnet ]

  airflow:
    image: apache/airflow:2.9.3
    container_name: airflow
    depends_on: [ spark ]
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: >
      bash -lc "
      airflow db init &&
      airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com || true &&
      airflow webserver -p 8080 &
      airflow scheduler
      "
    networks: [ bdnet ]

  producer:
    image: python:3.11-slim
    container_name: producer
    depends_on: [ kafka ]
    working_dir: /app
    volumes:
      - ./producer:/app
    command: [ "bash", "-lc", "pip install -r requirements.txt && python produce_logs.py" ]
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=logs_raw
      - RATE=150
    networks: [ bdnet ]

networks:
  bdnet:

volumes:
  namenode:
  datanode:
