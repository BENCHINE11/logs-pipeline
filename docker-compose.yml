services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks: [ bdnet ]

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on: [ zookeeper ]
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    networks: [ bdnet ]

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=bd
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
    networks: [ bdnet ]

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CLUSTER_NAME=bd
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on: [ namenode ]
    ports:
      - "9864:9864"
    volumes:
      - datanode:/hadoop/dfs/data
    networks: [ bdnet ]

  spark:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark
    user: "0"
    depends_on: [ kafka, namenode, datanode ]
    working_dir: /app
    volumes:
      - ./spark-streaming:/app/spark-streaming
      - ./spark-batch:/app/spark-batch
      - ./sql:/app/sql
      - ./.cs-cache:/root/.cache/coursier
      - ./.sbt-cache:/root/.sbt
      - ./.ivy-cache:/root/.ivy2
      - ./.ivy-cache:/home/spark/.ivy2
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=logs_raw
      - HDFS_OUT=hdfs://namenode:8020/datalake/logs/curated
      - CHECKPOINT=hdfs://namenode:8020/datalake/logs/_chk/curated
    command: [ "bash", "-lc", "sleep infinity" ]
    ports:
      - "4040:4040"
    networks: [ bdnet ]

  airflow:
    image: apache/airflow:2.9.3
    container_name: airflow
    depends_on: [ spark ]
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: >
      bash -lc "
      airflow db init &&
      airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com || true &&
      airflow webserver -p 8080 &
      airflow scheduler
      "
    networks: [ bdnet ]

  producer:
    image: python:3.11-slim
    container_name: producer
    depends_on: [ kafka ]
    working_dir: /app
    volumes:
      - ./producer:/app
    command:
      - bash
      - -lc
      - |
        pip install -r requirements.txt
        python - <<'PY'
        import os, time
        from kafka import KafkaProducer
        bootstrap=os.getenv("KAFKA_BOOTSTRAP","kafka:9092")
        for i in range(60):
            try:
                KafkaProducer(bootstrap_servers=bootstrap).close()
                print("Kafka is READY:", bootstrap)
                break
            except Exception as e:
                print("Waiting for Kafka...", e)
                time.sleep(2)
        else:
            raise SystemExit("Kafka not ready after 120s")
        PY
        python produce_logs.py
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=logs_raw
      - RATE=150
    networks: [ bdnet ]

  dashboard:
    image: python:3.11-slim
    container_name: dashboard
    working_dir: /app
    volumes:
      - ./dashboard:/app
      - ./analytics_local:/data/analytics
    ports:
      - "8501:8501"
    command: >
      bash -lc "pip install -r requirements.txt &&
                streamlit run streamlit_app.py --server.address=0.0.0.0 --server.port=8501"
    depends_on: [ namenode ]
    networks: [ bdnet ]

networks:
  bdnet:

volumes:
  namenode:
  datanode:
